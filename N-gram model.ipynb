{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import pickle\n",
    "import jax\n",
    "jax.config.update('jax_platforms', 'cpu')\n",
    "import numpy\n",
    "import trax.fastmath.numpy as np\n",
    "import random \n",
    "import trax\n",
    "from trax import fastmath\n",
    "from trax import layers as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "lines = []\n",
    "for file in os.listdir('shakespeare_data'):\n",
    "    f = open('shakespeare_data/' + file,'r')\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            lines.append(line.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and eval\n",
    "train_lines = lines[:-2000]\n",
    "test_lines = lines[-2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a line to a tensor\n",
    "def line_to_tensor(line, EOS_int=1):\n",
    "    char_tensor = []\n",
    "    for char in line:\n",
    "        char_tensor.append(ord(char))\n",
    "    char_tensor.append(EOS_int)\n",
    "    return char_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for data generator\n",
    "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
    "    batch = []\n",
    "    line_indexes = [*range(len(data_lines))]\n",
    "\n",
    "    if shuffle:\n",
    "        numpy.random.shuffle(line_indexes)\n",
    "    len_batch = 0\n",
    "    index = 0\n",
    "    while True:\n",
    "        while len_batch < batch_size:\n",
    "            if index == len(data_lines):\n",
    "                index = 0\n",
    "                if shuffle:\n",
    "                    numpy.random.shuffle(data_lines)\n",
    "            if len(data_lines[index]) < max_length:\n",
    "                batch.append(data_lines[index])\n",
    "                len_batch += 1\n",
    "            index+=1\n",
    "        tensors = []\n",
    "        for line in batch:\n",
    "            tensors.append(line_to_tensor(line))\n",
    "        padded_tensor = []\n",
    "        mask = []\n",
    "        for tensor in tensors:\n",
    "            if len(tensor) < max_length:\n",
    "                padded_tensor.append(tensor + [0]*(max_length - len(tensor)))\n",
    "                mask.append([1]*len(tensor) + [0]*(max_length - len(tensor)))\n",
    "            else:\n",
    "                padded_tensor.append(tensor)\n",
    "                mask.append([1]*max_length)\n",
    "        padded_tensor = np.array(padded_tensor)\n",
    "        mask = np.array(mask)\n",
    "        yield padded_tensor,padded_tensor,mask\n",
    "        len_batch = 0\n",
    "        batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def GRU_Model(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    model = tl.Serial(\n",
    "      tl.ShiftRight(mode=mode),\n",
    "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), \n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], \n",
    "      tl.Dense(n_units=vocab_size), \n",
    "      tl.LogSoftmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = GRU_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "from trax.supervised import training\n",
    "import itertools\n",
    "def train_model(model, data_generator, batch_size=32, max_length=64, lines=train_lines, eval_lines=test_lines, n_steps=5000): \n",
    "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "    \n",
    "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "   \n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=infinite_train_generator,  \n",
    "        loss_layer=tl.CrossEntropyLoss(),  \n",
    "        optimizer=trax.optimizers.Adam(0.001)\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=infinite_eval_generator,\n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    "        n_eval_batches=3  \n",
    "    )\n",
    "    \n",
    "    training_loop = training.Loop(model,\n",
    "                                  train_task,\n",
    "                                  eval_tasks=eval_task)\n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/.local/lib/python3.8/site-packages/jax/_src/lib/xla_bridge.py:555: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not write evaluation metrics, because output_dir is None.\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step      1: Total number of trainable weights: 3411200\n",
      "Step      1: Ran 1 train steps in 12.96 secs\n",
      "Step      1: train CrossEntropyLoss |  5.54513550\n",
      "Step      1: eval  CrossEntropyLoss |  5.54091152\n",
      "Step      1: eval          Accuracy |  0.16486038\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    100: Ran 99 train steps in 212.41 secs\n",
      "Step    100: train CrossEntropyLoss |  3.37593198\n",
      "Step    100: eval  CrossEntropyLoss |  2.88648979\n",
      "Step    100: eval          Accuracy |  0.18967202\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    200: Ran 100 train steps in 212.30 secs\n",
      "Step    200: train CrossEntropyLoss |  2.73634338\n",
      "Step    200: eval  CrossEntropyLoss |  2.57715742\n",
      "Step    200: eval          Accuracy |  0.26294392\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    300: Ran 100 train steps in 205.81 secs\n",
      "Step    300: train CrossEntropyLoss |  2.42932844\n",
      "Step    300: eval  CrossEntropyLoss |  2.40837733\n",
      "Step    300: eval          Accuracy |  0.30406312\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    400: Ran 100 train steps in 210.49 secs\n",
      "Step    400: train CrossEntropyLoss |  2.29956889\n",
      "Step    400: eval  CrossEntropyLoss |  2.25661659\n",
      "Step    400: eval          Accuracy |  0.32918801\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    500: Ran 100 train steps in 204.25 secs\n",
      "Step    500: train CrossEntropyLoss |  2.14483786\n",
      "Step    500: eval  CrossEntropyLoss |  2.21296199\n",
      "Step    500: eval          Accuracy |  0.35392117\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    600: Ran 100 train steps in 191.13 secs\n",
      "Step    600: train CrossEntropyLoss |  2.09189367\n",
      "Step    600: eval  CrossEntropyLoss |  2.17534542\n",
      "Step    600: eval          Accuracy |  0.35854542\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    700: Ran 100 train steps in 206.49 secs\n",
      "Step    700: train CrossEntropyLoss |  2.04490018\n",
      "Step    700: eval  CrossEntropyLoss |  2.03889807\n",
      "Step    700: eval          Accuracy |  0.39205561\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    800: Ran 100 train steps in 191.01 secs\n",
      "Step    800: train CrossEntropyLoss |  1.94180095\n",
      "Step    800: eval  CrossEntropyLoss |  1.94956783\n",
      "Step    800: eval          Accuracy |  0.41807870\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step    900: Ran 100 train steps in 185.29 secs\n",
      "Step    900: train CrossEntropyLoss |  1.88477659\n",
      "Step    900: eval  CrossEntropyLoss |  1.92630545\n",
      "Step    900: eval          Accuracy |  0.41380001\n",
      "Did not save checkpoint as output_dir is None\n",
      "\n",
      "Step   1000: Ran 100 train steps in 101.11 secs\n",
      "Step   1000: train CrossEntropyLoss |  1.82161379\n",
      "Step   1000: eval  CrossEntropyLoss |  1.86459756\n",
      "Step   1000: eval          Accuracy |  0.42828162\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "training_loop = train_model(GRU_Model(), data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the log perplexity\n",
    "def test_model(preds, target):\n",
    "    total_log_ppx = np.sum(preds * tl.one_hot(target, preds.shape[-1]),axis= -1)\n",
    "    non_pad = 1.0 - np.equal(target, 0)          \n",
    "    ppx = total_log_ppx * non_pad                   \n",
    "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of your model are respectively 2.7764776 16.062344\n"
     ]
    }
   ],
   "source": [
    "# Get the log perplexity for a batch\n",
    "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
    "preds = training_loop.eval_model(batch[0])\n",
    "log_ppx = test_model(preds, batch[1])\n",
    "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoul to caniolanusus\n"
     ]
    }
   ],
   "source": [
    "# Function to sample from the gumbel distribution\n",
    "def gumbel_sample(log_probs, temperature=1.0):\n",
    "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next characters\n",
    "def predict(num_chars, prefix):\n",
    "    inp = [ord(c) for c in prefix]\n",
    "    result = [c for c in prefix]\n",
    "    max_len = len(prefix) + num_chars\n",
    "    for _ in range(num_chars):\n",
    "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
    "        outp = training_loop.eval_model(cur_inp[None, :]) \n",
    "        next_char = gumbel_sample(outp[0, len(inp)])\n",
    "        inp += [int(next_char)]\n",
    "       \n",
    "        if inp[-1] == 1:\n",
    "            break  # EOS\n",
    "        result.append(chr(int(next_char)))\n",
    "    \n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king henry serviand blear what me fair hum\n"
     ]
    }
   ],
   "source": [
    "# Predict the next characters for a given prefix\n",
    "print(predict(32, \"king henry\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
